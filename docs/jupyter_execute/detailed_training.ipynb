{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```{eval-rst}\n",
    ".. role:: nge-yellow\n",
    "```\n",
    "{nge-yellow}`Detailed Training With a Custom Training Function`\n",
    "==================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For more fine grain control of the training process, we must define out own train function. This requires a bit more work, but allows much more flexibility. The training function must perform all initalization, dataloading and hyperparameter tuning. We will break down each step outside the function, then compile it all together at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports\n",
    "We must first import each package necessary for training. SKOOTS tries to take a functional approach at training. It not exactly in line with functional programing best practices, but avoids you from going into a hell of inheritance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Python standard library\n",
    "from functools import partial\n",
    "import os.path\n",
    "from typing import Tuple, Callable, Dict\n",
    "\n",
    "# Pytorch imports\n",
    "import torch.optim.lr_scheduler\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch import Tensor\n",
    "\n",
    "# Skoots imports\n",
    "from skoots.train.dataloader import dataset, MultiDataset, skeleton_colate\n",
    "from skoots.train.sigma import Sigma\n",
    "from skoots.train.loss import tversky\n",
    "from skoots.train.merged_transform import merged_transform_3D, background_transform_3D\n",
    "from skoots.train.engine import engine\n",
    "from skoots.train.setup import setup_process, cleanup, find_free_port"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define a Training Function\n",
    "We need to define 3 mandatory inputs: ```rank```, ```port```, and ```world_size```. Starting in reverse, ```world_size``` is the total number of devices to run distributed training on. If you have two GPU's in one machine, then your world size would be 2. ```port``` is the port of a local web server by which to run distributed training. ```rank``` is the process number. So for a ```world_size``` of 2, we would get two process, one where ```rank=0``` and one with ```rank=1```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(rank: str,\n",
    "          port: str,\n",
    "          world_size: int,\n",
    "          model: nn.Module\n",
    "          ) -> None:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It is therefore reasonable that we may pass the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rank = '0'\n",
    "port = '51234'\n",
    "world_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Lets also set up some other constants necessary for training, namely the anisotropy and vector scaling parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anisotropy = (1.0, 1.0, 5.0)\n",
    "vector_scale = (60, 60, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## DDP Initalization\n",
    "To run this in DDP, we need ot setup the process, send the model to the GPU, and wrap it in a DDP wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "setup_process(rank, world_size, port, backend='nccl')\n",
    "\n",
    "device = f'cuda:{rank}'\n",
    "model = model.to(device)\n",
    "model = torch.nn.parallel.DistributedDataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Loading\n",
    "We now need to load our data. Using the SKOOTS dataloader, and skeleton collate function, we can ensure our data is properly handled. First Lets define our augmentations. We can either write our own, or use the pre-built augmentations (which is recommended). See our tutorial on augmentations for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "augmentations: Callable[[Dict[str, Tensor]], Dict[str, Tensor]] = partial(merged_transform_3D,\n",
    "                                                                          bake_skeleton_anisotropy=anisotropy,\n",
    "                                                                          device=device)\n",
    "augmentations_background: Callable[[Dict[str, Tensor]], Dict[str, Tensor]] = partial(background_transform_3D, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can now load our data from our training and validation datasets. To do this we use the SKOOTS dataloaders. See the tutorial on dataloading for more details. If you have multiple datasets and therefore multiple dataloaders, you can use the ```MultiDataset``` class to merge two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data = dataset(path='./train', transforms=augmentations, sample_per_image=32, device=device, pad_size=100)\n",
    "background_data = dataset(path='./background', transforms=augmentations_background, sample_per_image=32, device=device, pad_size=100)\n",
    "merged_train_data = MultiDataset(train_data, background_data)\n",
    "\n",
    "validation_data = dataset(path='./validation', transforms=augmentations, sample_per_image=32, device=device, pad_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we need to create a distributed sampler to ensure each dataset is sampled appropriatly. This is necessary by pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_sampler = torch.utils.data.distributed.DistributedSampler(train_data)\n",
    "validation_sampler = torch.utils.data.distributed.DistributedSampler(validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now wrap our datasets in pytorch dataloaders to allow for automatic batching and collation! We must use the DDP training sampler and the SKOOTS colate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(merged_train_data, num_workers=0, batch_size=2, sampler=train_sampler, collate_fn=skeleton_colate)\n",
    "validation_dataloader = DataLoader(validation_data, num_workers=0, batch_size=2, sampler=validation_sampler, collate_fn=skeleton_colate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Embedding Distance Penalty\n",
    "To calculate the embedding loss, SKOOTS needs a value reflecting the distance penalty. We term this value sigma, and there are different values for x, y, and z. We use the ```skoots.train.sigma``` library to construct an object allowing us to decay the sigma at set epochs by a multiplier. See the API reference for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "initial_sigma = torch.tensor([20., 20., 20.], device=device)\n",
    "a = {'multiplier': 0.66, 'epoch': 200}\n",
    "b = {'multiplier': 0.66, 'epoch': 800}\n",
    "c = {'multiplier': 0.66, 'epoch': 1500}\n",
    "d = {'multiplier': 0.5, 'epoch': 20000}\n",
    "f = {'multiplier': 0.5, 'epoch': 20000}\n",
    "sigma = Sigma([a, b, c, d, f], initial_sigma, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Hyperparameters\n",
    "We now must define a dictionary of hyperparameters which will be passed as keyword argumetns to a further training engine, which handles loss calculation and backpropagaition. These hyperparameters may be saved along with the model weights, ensuring replicability. Each key in this dictionary must be filled (even with a None) and spelled as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "constants = {\n",
    "    'model': model,  # UNet model\n",
    "    'vector_scale': vector_scale,\n",
    "    'anisotropy': anisotropy,\n",
    "    'lr': 5e-4,  # learning rate\n",
    "    'wd': 1e-6,  # optimizer weight decay\n",
    "    'optimizer': partial(torch.optim.AdamW, eps=1e-16),\n",
    "    'scheduler': partial(torch.optim.lr_scheduler.CosineAnnealingWarmRestarts, T_0=10000+ 1),\n",
    "    'sigma': sigma,\n",
    "    'loss_embed': tversky(alpha=0.25, beta=0.75, eps=1e-8, device=device), # Loss functions, see API reference for more details\n",
    "    'loss_prob': tversky(alpha=0.5, beta=0.5, eps=1e-8, device=device),\n",
    "    'loss_skele': tversky(alpha=0.5, beta=1.5, eps=1e-8, device=device),\n",
    "    'epochs': 10000,  # total number of training epochs\n",
    "    'device': device,\n",
    "    'train_data': train_dataloader,\n",
    "    'val_data': validation_dataloader,\n",
    "    'train_sampler': train_sampler,\n",
    "    'test_sampler': validation_sampler,\n",
    "    'distributed': True,\n",
    "    'mixed_precision': True,  # can use automatic mixed precision which may speed up training\n",
    "    'rank': rank,\n",
    "    'savepath': './models',  # where to save the model at the end\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training Engine\n",
    "We are now ready to pass the hyperparameters to the training engine, which handles the forward and backward passes. We can optionally provide a tensorboard writer to track the training. The training engine will run for the predetermined number of epochs then return the model state dict, optimizer state dict, and loss at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter() if rank == 0 else None\n",
    "model_state_dict, optimizer_state_dict, avg_loss = engine(writer=writer, verbose=True, force=True, **constants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Saving\n",
    "Now that training is done, we save the model and its hyperparams. We only need to do this for one process, as the model weights are shared via DDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if rank == 0:\n",
    "    for k in constants: # Some hyperparams cannot be saved as is, so we simply get a string representation. This is usually good enough.\n",
    "        if k in ['model', 'train_data', 'val_data', 'train_sampler', 'test_sampler', 'loss_embed', 'loss_prob']:\n",
    "            constants[k] = str(constants[k])\n",
    "\n",
    "    # Save the weights of the model and optimizer\n",
    "    constants['model_state_dict'] = model_state_dict\n",
    "    constants['optimizer_state_dict'] = optimizer_state_dict\n",
    "\n",
    "\n",
    "# Save the model to a file!\n",
    "torch.save(constants,f'./models/{os.path.split(writer.log_dir)[-1]}.trch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Cleanup\n",
    "We now must run a mandatory cleanup for our process. This is mandated by pytorch DDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cleanup(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## All Together\n",
    "And now we are done! This should train your entire model. The entire script is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(rank: str,\n",
    "          port: str,\n",
    "          world_size: int,\n",
    "          model: nn.Module\n",
    "          ) -> None:\n",
    "    # setup\n",
    "    setup_process(rank, world_size, port, backend='nccl')\n",
    "\n",
    "    device = f'cuda:{rank}'\n",
    "    model = model.to(device)\n",
    "    model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "\n",
    "    # Augmentations\n",
    "    augmentations: Callable[[Dict[str, Tensor]], Dict[str, Tensor]] = partial(merged_transform_3D,\n",
    "                                                                          bake_skeleton_anisotropy=anisotropy,\n",
    "                                                                          device=device)\n",
    "    augmentations_background: Callable[[Dict[str, Tensor]], Dict[str, Tensor]] = partial(background_transform_3D, device=device)\n",
    "\n",
    "    # Load data and place in dataloader\n",
    "    train_data = dataset(path='./train', transforms=augmentations, sample_per_image=32, device=device, pad_size=100)\n",
    "    background_data = dataset(path='./background', transforms=augmentations_background, sample_per_image=32, device=device, pad_size=100)\n",
    "    merged_train_data = MultiDataset(train_data, background_data)\n",
    "\n",
    "    validation_data = dataset(path='./validation', transforms=augmentations, sample_per_image=32, device=device, pad_size=100)\n",
    "\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(train_data)\n",
    "    validation_sampler = torch.utils.data.distributed.DistributedSampler(validation_data)\n",
    "    train_dataloader = DataLoader(merged_train_data, num_workers=0, batch_size=2, sampler=train_sampler, collate_fn=skeleton_colate)\n",
    "    validation_dataloader = DataLoader(validation_data, num_workers=0, batch_size=2, sampler=validation_sampler, collate_fn=skeleton_colate)\n",
    "\n",
    "    # Define embedding distance penalty (sigma)\n",
    "    initial_sigma = torch.tensor([20., 20., 20.], device=device)\n",
    "    a = {'multiplier': 0.66, 'epoch': 200}\n",
    "    b = {'multiplier': 0.66, 'epoch': 800}\n",
    "    c = {'multiplier': 0.66, 'epoch': 1500}\n",
    "    d = {'multiplier': 0.5, 'epoch': 20000}\n",
    "    f = {'multiplier': 0.5, 'epoch': 20000}\n",
    "    sigma = Sigma([a, b, c, d, f], initial_sigma, device)\n",
    "\n",
    "    # Define constants for training engine\n",
    "    constants = {\n",
    "        'model': model,  # UNet model\n",
    "        'vector_scale': vector_scale,\n",
    "        'anisotropy': anisotropy,\n",
    "        'lr': 5e-4,  # learning rate\n",
    "        'wd': 1e-6,  # optimizer weight decay\n",
    "        'optimizer': partial(torch.optim.AdamW, eps=1e-16),\n",
    "        'scheduler': partial(torch.optim.lr_scheduler.CosineAnnealingWarmRestarts, T_0=10000+ 1),\n",
    "        'sigma': sigma,\n",
    "        'loss_embed': tversky(alpha=0.25, beta=0.75, eps=1e-8, device=device), # Loss functions, see API reference for more details\n",
    "        'loss_prob': tversky(alpha=0.5, beta=0.5, eps=1e-8, device=device),\n",
    "        'loss_skele': tversky(alpha=0.5, beta=1.5, eps=1e-8, device=device),\n",
    "        'epochs': 10000,  # total number of training epochs\n",
    "        'device': device,\n",
    "        'train_data': train_dataloader,\n",
    "        'val_data': validation_dataloader,\n",
    "        'train_sampler': train_sampler,\n",
    "        'test_sampler': validation_sampler,\n",
    "        'distributed': True,\n",
    "        'mixed_precision': True,  # can use automatic mixed precision which may speed up training\n",
    "        'rank': rank,\n",
    "        'savepath': './models',  # where to save the model at the end\n",
    "    }\n",
    "\n",
    "    # tensorboard logging\n",
    "    writer = SummaryWriter() if rank == 0 else None\n",
    "\n",
    "    # train model from hyperparams\n",
    "    model_state_dict, optimizer_state_dict, avg_loss = engine(writer=writer, verbose=True, force=True, **constants)\n",
    "\n",
    "    # Convert hyperparams to string for saving\n",
    "    if rank == 0:\n",
    "        for k in constants: # Some hyperparams cannot be saved as is, so we simply get a string representation. This is usually good enough.\n",
    "            if k in ['model', 'train_data', 'val_data', 'train_sampler', 'test_sampler', 'loss_embed', 'loss_prob']:\n",
    "                constants[k] = str(constants[k])\n",
    "\n",
    "        # Save the weights of the model and optimizer to constants dict\n",
    "        constants['model_state_dict'] = model_state_dict\n",
    "        constants['optimizer_state_dict'] = optimizer_state_dict\n",
    "\n",
    "\n",
    "    # Save the model to a file!\n",
    "    torch.save(constants,f'./models/{os.path.split(writer.log_dir)[-1]}.trch')\n",
    "\n",
    "    # cleanup the DDP process\n",
    "    cleanup(rank)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}