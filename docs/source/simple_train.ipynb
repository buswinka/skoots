{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "```{eval-rst}\n",
    ".. role:: nge-green\n",
    "```\n",
    "{nge-green}`Training a SKOOTS Model`\n",
    "==================================="
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This guide is for people unfamiliar with YACS and the command line. The SKOOTS library provides necessary pre-written evaluation functions which make it easy to train a segmentation model. We typically do this through configuration files, which is then used to define a training run. This all happens through the command line. We will first show you how to prepare your data, construct a configuration file, train using the command line. For details on the training script, how it works, and how to hack it, please see the detailed training\n",
    "The built in training scripts uses pytorch's DistributedDataParallel by default with an 'nvcc' communication server, so unfortunately\n",
    "requires an Nvidia GPU.\n",
    "\n",
    "## Prepare Your Data\n",
    "We start by preparing our data. SKOOTS expects training images to be large tiff images with associated masks. SKOOTS will associate the mask to the image by its filename and tag. Images may be named whatever you'd like, for example: ``` training_data.tif ```. The associate labels must therefore be named as such: ```training_data.labels.tif ```. The background of each label must be zero."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Precompute Ground Truth Skeletons\n",
    "Once our training data is in an appropriate place, we must pre-compute the ground truth skeletons. This generates a seperate file and need only happen once. SKOOTS provides necessary utility functions for creating the skeletons, however an explicit script must be created for your own data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "from typing import Dict\n",
    "\n",
    "from skoots.train.generate_skeletons import calculate_skeletons\n",
    "\n",
    "training_directory = './train'  # base directory containing all our data.\n",
    "\n",
    "# Sometimes skeletons are 'weird' looking due to anisotropy.\n",
    "# Scaling the image can a predetermined amount can help with this.\n",
    "# This may need trial and error to get skeletons which are easily predicable\n",
    "scale_factors = torch.tensor([1, 1, 0.5])\n",
    "\n",
    "# Loop over all the mask files\n",
    "for f in glob.glob(training_directory + '/*.labels.tif'):\n",
    "    masks: np.ndarray = io.imread(f) # will read in as an uint16 numpy array with shape [Z, X, Y]\n",
    "    masks = torch.from_numpy(masks.astype(np.int32))  # pytorch cannot import uint16, convert to 32bit int instead.\n",
    "    masks = masks.permute(1, 2, 0)  # the script expects the tensor to be [X, Y, Z]\n",
    "    skeletons: Dict[int, torch.Tensor] = calculate_skeletons(masks, scale_factors)  # calculate the skeletons\n",
    "\n",
    "    f = f[:-11:] # get rid of '.labels.tif'\n",
    "    torch.save(skeletons, f + '.skeletons.trch')  # IMPORTANT! skeletons must be saved with this extension and tag!!!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now have three files for each training image. The image: ```train_image.tif```, the instance masks: ```train_image.labels.tif```, and the precomputed skeletons: ```train_image.skeletons.trch```. Precomputing the skeletons need only happen once, saving training time, as it can be an expensive procedure. All three must be present in the same folder for training! In this case, lets put them in ```./train```  . We may do the same process for validation images and put them in ```./validate```. You may also want to provide a set of background images by which to training the model to be robust against. These images have no masks and therefore no skeletons. We'll put these in the folder ```./background```.\n",
    "\n",
    "We can also do this through the skoots CLI! We simply put all training image masks in a single folder, and run this in the terminal:\n",
    "```bash\n",
    "skoots --skeletonize_train_data \"path/to/training/data\" --downscaleXY 1 --downscaleZ 0.5\n",
    "```\n",
    "\n",
    "This command will create a bunch of files with the extension ```*.skeletons.trch``` with the same filename as each training mask.\n",
    "\n",
    "## Configure your training\n",
    "\n",
    "SKOOTS uses YACS to configure training of our models. YACS is a python extension which allows us to define all variables which might influence training in a single text file: ```config.yaml```. SKOOTS has a defualt configuration for everything, and they are defined in the file ```skoots/config.py```. YACS uses these defaults to fill in everything you dont want to. Each line in the config file, maps to a YAML file which could be used to specify the configuration. For instance, in config.py, ```_C.TRAIN.NUM_EPOCHS``` would be set in a YAML file that looks like this:\n",
    "```\n",
    "TRAIN:\n",
    "    NUM_EPOCHS: 100\n",
    "```\n",
    "\n",
    "There are other training parameters too. Say we want to set the training batch size, defined in config.py as ```_C.TRAIN.TRAN_BATCH_SIZE```. We could set both configurations as:\n",
    "\n",
    "```\n",
    "TRAIN:\n",
    "    NUM_EPOCHS: 100\n",
    "    TRAIN_BATCH_SIZE: 2\n",
    "```\n",
    "\n",
    "Notice how the YAML file groups similar configs together. To set up a pretty minimalistic training run, your config file might look like this:\n",
    "\n",
    "```\n",
    "TRAIN:\n",
    "    PRETRAINED_MODEL_PATH: ['path/to/pretrained_model.trch']\n",
    "    TRAIN_DATA_DIR: ['path/to/train/data', '/path/to/more/train/data]  # can specifiy multiple sources of train data\n",
    "    VALIDATION_DATA_DIR: ['path/to/validation/data', '/path/to/more/validation/data] # similarly, validation data\n",
    "\n",
    "```\n",
    "\n",
    "For reference, the entirety of the config.py file looks like this:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from yacs.config import CfgNode as CN\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Training config definition\n",
    "# -----------------------------------------------------------------------------\n",
    "_C = CN()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# System\n",
    "# -----------------------------------------------------------------------------\n",
    "_C.SYSTEM = CN()\n",
    "\n",
    "_C.SYSTEM.NUM_GPUS = 2  # number of available NVIDIA GPU's to train on\n",
    "_C.SYSTEM.NUM_CPUS = 1  # How many CPU's do you have? Might be more than 1 when doing distributed training\n",
    "\n",
    "# Define a BISM Model\n",
    "_C.MODEL = CN()\n",
    "_C.MODEL.ARCHITECTURE = 'bism_unext'  # name of bism model\n",
    "_C.MODEL.IN_CHANNELS = 1  # number of input color channels, 1 for grayscale, 3 for rgb\n",
    "_C.MODEL.OUT_CHANNELS = 32  # output of model backbone, but not skoots\n",
    "_C.MODEL.DIMS = [32, 64, 128, 64, 32]   # number of channels at each level of a unet\n",
    "_C.MODEL.DEPTHS = [2, 2, 2, 2, 2]  # number of computational blocks at each level of the unet\n",
    "_C.MODEL.KERNEL_SIZE = 7  # kernel size of each convolution in unet\n",
    "_C.MODEL.DROP_PATH_RATE = 0.0\n",
    "_C.MODEL.LAYER_SCALE_INIT_VALUE = 1.\n",
    "_C.MODEL.ACTIVATION = 'gelu'\n",
    "_C.MODEL.BLOCK = 'block3d'  # computational blocks, see bism for all available\n",
    "_C.MODEL.CONCAT_BLOCK = 'concatconv3d'  # concatenation bocks for skip connections\n",
    "_C.MODEL.UPSAMPLE_BLOCK = 'upsamplelayer3d' # upsample operation\n",
    "_C.MODEL.NORMALIZATION='layernorm'  # normalization layer, could be batch norm...\n",
    "\n",
    "# Training Configurations\n",
    "_C.TRAIN = CN()\n",
    "_C.TRAIN.DISTRIBUTED = True  # do we use pytorch data distributed parallel?\n",
    "_C.TRAIN.PRETRAINED_MODEL_PATH = [\n",
    "    '/home/chris/Dropbox (Partners HealthCare)/trainMitochondriaSegmentation/models/Oct20_11-54-51_CHRISUBUNTU.trch'\n",
    "]  # path to pretrained model\n",
    "\n",
    "# embedding loss function and their constructor keywords\n",
    "_C.TRAIN.LOSS_EMBED = 'tversky'  # could also be \"soft_cldice\"\n",
    "_C.TRAIN.LOSS_EMBED_KEYWORDS = ['alpha', 'beta', 'eps']  # kwargs for the loss function\n",
    "_C.TRAIN.LOSS_EMBED_VALUES = [0.25, 0.75, 1e-8]  # values for each kwarg\n",
    "\n",
    "# Semantic mask loss function and their constructor keywords\n",
    "_C.TRAIN.LOSS_PROBABILITY = 'tversky'\n",
    "_C.TRAIN.LOSS_PROBABILITY_KEYWORDS = ['alpha', 'beta', 'eps']\n",
    "_C.TRAIN.LOSS_PROBABILITY_VALUES = [0.5, 0.5, 1e-8]\n",
    "\n",
    "# Skeleton mask loss function and their constructor keywords\n",
    "_C.TRAIN.LOSS_SKELETON = 'tversky'\n",
    "_C.TRAIN.LOSS_SKELETON_KEYWORDS = ['alpha', 'beta', 'eps']\n",
    "_C.TRAIN.LOSS_SKELETON_VALUES = [0.5, 1.5, 1e-8]\n",
    "\n",
    "# We sum the loss values of each part together, scaled by some factor set here\n",
    "_C.TRAIN.LOSS_EMBED_RELATIVE_WEIGHT = 1.0\n",
    "_C.TRAIN.LOSS_PROBABILITY_RELATIVE_WEIGHT = 1.0\n",
    "_C.TRAIN.LOSS_SKELETON_RELATIVE_WEIGHT = 1.0\n",
    "\n",
    "# We may not consider each loss until a certain epoch. This may be\n",
    "# because some tasks are hard to learn at the start, and must only be considered later\n",
    "# roughly does this:\n",
    "# loss_embed = loss_embed if cfg.TRAIN.LOSS_EMBED_START_EPOCH < epoch else torch.tensor(0)\n",
    "# ...same for skeleton and semantic mask...\n",
    "_C.TRAIN.LOSS_EMBED_START_EPOCH = -1\n",
    "_C.TRAIN.LOSS_PROBABILITY_START_EPOCH = -1  # sets the epoch where the semantic mask loss is added to\n",
    "_C.TRAIN.LOSS_SKELETON_START_EPOCH = 10\n",
    "\n",
    "# Train, Validation, and Background data share similar syntax\n",
    "# *_DATA_DIR is a list of locations to look for training data, can have multiple sources\n",
    "# *_SAMPLE_PER_IMAGE is a list of the number of times to sample each image in an epoch, must be the same size as *_DATA_DIR\n",
    "# *_BATCH_SIZE is the batch size\n",
    "# BACKGROUND are just images of nothing, and do not need associated label files\n",
    "_C.TRAIN.TRAIN_DATA_DIR = [\n",
    "    '/home/chris/Dropbox (Partners HealthCare)/trainMitochondriaSegmentation/data/unscaled/train']\n",
    "_C.TRAIN.TRAIN_SAMPLE_PER_IMAGE = [32]\n",
    "_C.TRAIN.TRAIN_BATCH_SIZE = 2\n",
    "_C.TRAIN.VALIDATION_DATA_DIR = [\n",
    "    '/home/chris/Dropbox (Partners HealthCare)/trainMitochondriaSegmentation/data/unscaled/validate']\n",
    "_C.TRAIN.VALIDATION_SAMPLE_PER_IMAGE = [6]\n",
    "_C.TRAIN.VALIDATION_BATCH_SIZE = 1\n",
    "_C.TRAIN.BACKGROUND_DATA_DIR = [\n",
    "    '/home/chris/Dropbox (Partners HealthCare)/trainMitochondriaSegmentation/data/background']\n",
    "_C.TRAIN.BACKGROUND_SAMPLE_PER_IMAGE = [8]\n",
    "\n",
    "_C.TRAIN.STORE_DATA_ON_GPU = False  # Sends all training data to GPU for faster access\n",
    "\n",
    "# Sigma sets the distance penalty for embedding. Each number is in units of pixels\n",
    "# See the manuscript methods section for more detail\n",
    "_C.TRAIN.INITIAL_SIGMA = [20., 20., 20.]  # [X, Y, Z]\n",
    "_C.TRAIN.SIGMA_DECAY = [[0.66, 200], [0.66, 800], [0.66, 1500], [0.5, 20000], [0.5, 20000]] # List of sigma decays [[fraction, decay_epoch], ...], i.e sigma *= fraction if epoch > decay_epoch\n",
    "_C.TRAIN.NUM_EPOCHS = 10000  # total number of epoch to train\n",
    "_C.TRAIN.LEARNING_RATE = 5e-4  # optimizer learning rate\n",
    "_C.TRAIN.WEIGHT_DECAY = 1e-6  # optimizer weight decay\n",
    "_C.TRAIN.OPTIMIZER = 'adamw'  # Train optimizer. Valid are: 'Adam', 'AdamW', 'SGD',\n",
    "_C.TRAIN.OPTIMIZER_EPS = 1e-8  # Optimizer eps\n",
    "_C.TRAIN.SCHEDULER = 'cosine_annealing_warm_restarts'  # learning rate scheduler, currently this is the only implemented\n",
    "_C.TRAIN.SCHEDULER_T0 = 10000 + 1  # period of learning rate scheduler\n",
    "_C.TRAIN.MIXED_PRECISION = True  # train using pytorch automatic mixed precision AMP\n",
    "_C.TRAIN.N_WARMUP = 1500  # number of times to train on an inital example to warm up model.\n",
    "_C.TRAIN.SAVE_PATH = '/home/chris/Dropbox (Partners HealthCare)/trainMitochondriaSegmentation/models'  # where do we save the model, and intermediate files?\n",
    "_C.TRAIN.SAVE_INTERVAL = 100  # saves a snapshot of the model every SAVE_INTERVAL number of epochs\n",
    "_C.TRAIN.CUDNN_BENCHMARK = True  # sets torch.backends.cudnn.benchmark\n",
    "_C.TRAIN.AUTOGRAD_PROFILE = False  # sets torch.autograd.profiler.profile\n",
    "_C.TRAIN.AUTOGRAD_EMIT_NVTX = False # sets torch.autograd.profiles.emit_nvtx(enabled= * )\n",
    "_C.TRAIN.AUTOGRAD_DETECT_ANOMALY = False # sets torch.autograd.set_detect_anomaly( * )\n",
    "\n",
    "# Augmentation Configuration\n",
    "# these are self-explanatory and set the valid image augmentations for training\n",
    "# for more detail on usage, see skoots/train/merge_transform.py\n",
    "_C.AUGMENTATION = CN()\n",
    "_C.AUGMENTATION.CROP_WIDTH = 300\n",
    "_C.AUGMENTATION.CROP_HEIGHT = 300\n",
    "_C.AUGMENTATION.CROP_DEPTH = 20\n",
    "_C.AUGMENTATION.FLIP_RATE = 0.5\n",
    "_C.AUGMENTATION.BRIGHTNESS_RATE = 0.4\n",
    "_C.AUGMENTATION.BRIGHTNESS_RANGE = [-0.1, 0.1]\n",
    "_C.AUGMENTATION.NOISE_GAMMA = 0.1\n",
    "_C.AUGMENTATION.NOISE_RATE = 0.2\n",
    "_C.AUGMENTATION.CONTRAST_RATE = 0.33\n",
    "_C.AUGMENTATION.CONTRAST_RANGE = [0.75, 2.0]\n",
    "_C.AUGMENTATION.AFFINE_RATE = 0.66\n",
    "_C.AUGMENTATION.AFFINE_SCALE = [0.85, 1.1]\n",
    "_C.AUGMENTATION.AFFINE_YAW = [-180, 180]\n",
    "_C.AUGMENTATION.AFFINE_SHEAR = [-7, 7]\n",
    "_C.AUGMENTATION.SMOOTH_SKELETON_KERNEL_SIZE = (3, 3, 1)\n",
    "_C.AUGMENTATION.BAKE_SKELETON_ANISOTROPY = (1.0, 1.0, 3.0) # does not necessarily have to reflect data anisotropy\n",
    "_C.AUGMENTATION.N_SKELETON_MASK_DILATE = 1\n",
    "\n",
    "\n",
    "# Skoots Generics\n",
    "_C.SKOOTS = CN()\n",
    "# This sets the vector scaling of your data, and should roughly equate to the\n",
    "# max distance of any pixel to a skeleton.\n",
    "_C.SKOOTS.VECTOR_SCALING = (60, 60, 60 // 5)\n",
    "\n",
    "# this sets the anisotropy of your data,\n",
    "_C.SKOOTS.ANISOTROPY = (1.0, 1.0, 3.0)\n",
    "\n",
    "\n",
    "def get_cfg_defaults():\n",
    "    r\"\"\"Get a yacs CfgNode object with default values for my_project.\"\"\"\n",
    "    # Return a clone so that the defaults will not be altered\n",
    "    # This is for the \"local variable\" use pattern\n",
    "    return _C.clone()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train your model\n",
    "\n",
    "We are now able to train a model. Make sure all data is where it should be and that your configuration file is set and named something informative. Now we write in the terminal:\n",
    "\n",
    "```bash\n",
    "skoots-train --config-file \"my_config.yaml\"\n",
    "```\n",
    "\n",
    "SKOOTS will log intermediary steps in tensorboard for diagnoses, and save a file in ```_C.TRAIN.SAVE_PATH``` with the logdir name. All information on training is saved in this file, including the configuration, if you ever forget what model was trained for what dataset. To evaluate this model, in the terminal type:\n",
    "\n",
    " ```bash\n",
    " skoots --pretrained_checkpoint \"mymodel.trch\" --image \"inference_image.tif\"\n",
    " ```\n",
    "\n",
    "For more details on hot training works and how to extend it, see the detailed training example."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}