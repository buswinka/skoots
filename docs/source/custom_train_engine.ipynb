{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```{eval-rst}\n",
    ".. role:: nge-red\n",
    "```\n",
    "{nge-red}`Define a Custom Training Engine`\n",
    "==================================="
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the most control when training, it is possible to define your own training engine which handles the forward and backward pass. This is for those familiar with deep learning techniques and pytorch and therefore will offer less explanation than before.\n",
    ":::{note}\n",
    "For consistency with previous tutorials, we recommend not changing the names of any keyword arguments.\n",
    ":::\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import skoots.train.loss\n",
    "from skoots.train.utils import sum_loss, show_box_pred\n",
    "from skoots.train.sigma import Sigma\n",
    "from skoots.lib.vector_to_embedding import vector_to_embedding\n",
    "from skoots.lib.embedding_to_prob import baked_embed_to_prob\n",
    "\n",
    "from skoots.train.utils import mask_overlay, write_progress\n",
    "\n",
    "from typing import List, Tuple, Callable, Union, OrderedDict, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import trange\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from statistics import mean\n",
    "from torchvision.utils import flow_to_image, draw_keypoints, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim.swa_utils\n",
    "Dataset = Union[Dataset, DataLoader]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Function Definition and Keyword Arguments\n",
    "For a detailed understanding of each keyword argumetn, see the detailed training guide. These parameters should ideally be passed via a kwarg dict."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def engine(\n",
    "        model: nn.Module,\n",
    "        lr: float,\n",
    "        wd: float,\n",
    "        vector_scale: Tuple[int, int, int],\n",
    "        epochs: int,\n",
    "        optimizer: Optimizer,\n",
    "        scheduler,\n",
    "        sigma: Sigma,\n",
    "        loss_embed,\n",
    "        loss_prob,\n",
    "        loss_skele,\n",
    "        device: str,\n",
    "        savepath: str,\n",
    "        train_data: Dataset,\n",
    "        rank: int,\n",
    "        val_data: Optional[Dataset] = None,\n",
    "        train_sampler=None,\n",
    "        test_sampler=None,\n",
    "        writer=None,\n",
    "        verbose=False,\n",
    "        distributed=True,\n",
    "        mixed_precision=False,\n",
    "        n_warmup: int = 100,\n",
    "        force=False,\n",
    "        **kwargs,\n",
    ") -> Tuple[OrderedDict, OrderedDict, List[float]]:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initalization\n",
    "Here we initalize the optimizer, scheduler, and grade scaler for automatic mixed preciions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = optimizer(model.parameters(), lr=lr, weight_decay=wd)\n",
    "scheduler = scheduler(optimizer)\n",
    "scaler = GradScaler(enabled=mixed_precision)\n",
    "\n",
    "vector_scale = vector_scale.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stochastic Weight Averaging\n",
    "We may want to use Stochastic Weight Averaging to improve generalizability of the model. I am not convinced it does much however."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "swa_model = torch.optim.swa_utils.AveragedModel(model)\n",
    "swa_start = 100\n",
    "swa_scheduler = torch.optim.swa_utils.SWALR(optimizer, swa_lr=0.05)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving the Loss\n",
    "We can save the loss values of all training and validation outputs for writing to tensorboard later. This is optional, and only if you want logging."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save each loss value in a list...\n",
    "avg_epoch_loss = []\n",
    "avg_epoch_embed_loss = []\n",
    "avg_epoch_prob_loss = []\n",
    "avg_epoch_skele_loss = []\n",
    "\n",
    "avg_val_loss = []\n",
    "avg_val_embed_loss = []\n",
    "avg_val_prob_loss = []\n",
    "avg_val_skele_loss = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Warmup\n",
    "This is critical and worth more discussion. These models are incredibly hard to train from scratch. It is therefore usefull to over-train a model on one input which warms up the weights, and makes the model better positioned for general training. To do this, we do a mini version of our actual training loop as outlined below. The API reference for training can be found in the 'API Flow Guide` section of the documentation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Warmup... Get the first from train_data\n",
    "for images, masks, skeleton, skele_masks, baked in train_data:\n",
    "    pass\n",
    "\n",
    "warmup_range = trange(n_warmup, desc='Warmup: {}')\n",
    "for w in warmup_range:\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    with autocast(enabled=mixed_precision):  # Saves Memory!\n",
    "        out: Tensor = model(images)\n",
    "\n",
    "        # break the singular model output tensor into its respective components\n",
    "        probability_map: Tensor = out[:, [-1], ...]\n",
    "        vector: Tensor = out[:, 0:3:1, ...]\n",
    "        predicted_skeleton: Tensor = out[:, [-2], ...]\n",
    "\n",
    "        embedding: Tensor = vector_to_embedding(vector_scale, vector)\n",
    "        out: Tensor = baked_embed_to_prob(embedding, baked, sigma(0))\n",
    "\n",
    "        # Loss functions are nn.Modules which have their own forward pass.\n",
    "        _loss_embed = loss_embed(out, masks.gt(0).float())  # out = [B, 2/3, X, Y, Z?]\n",
    "        _loss_prob = loss_prob(probability_map, masks.gt(0).float())\n",
    "        _loss_skeleton = loss_skele(predicted_skeleton, skele_masks.gt(\n",
    "            0).float())\n",
    "\n",
    "        # We may want to weight one loss more than another. Therefore we can multiply by a set amount. 1-1-1 is usually good though.\n",
    "        loss = _loss_embed + (1 * _loss_prob) + (1 * _loss_skeleton)\n",
    "\n",
    "        warmup_range.desc = f'{loss.item()}'\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Outputs to Loss\n",
    "It is worth discussing how we go from the embedding vectors, which lie between 1 and 1, to the embedding loss. Our vectors $V$ for dimension $l$ at locations $ijk$ must be scaled by our vector scaling parameter $\\Gamma$ for dimension $l$. Formally: $V_{l}^{ijk} = V_l^{ijk} + \\Gamma_{l}$ for $l \\in [x, y, z]$.\n",
    "\n",
    "We now can apply these vectors to their own index $ijk$ to determine our embedding $E_{ijk}$. Formally: $E_{ijk} = i+\\Gamma_x, j+\\Gamma_y, k + \\Gamma_z$. This operation is performed by ```skoots.lib.vector_to_embedding.vector_to_embedding```. From here we can calculate a score based on a baked skeleton tensor. A baked skeleton tensor $S$ in $ijk$ is defined as the closest skeleton of an instance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train Step...\n",
    "epoch_range = trange(epochs, desc=f'Loss = {1.0000000}') if rank == 0 else range(epochs)\n",
    "for e in epoch_range:\n",
    "    _loss, _embed, _prob, _skele = [], [], [], []\n",
    "\n",
    "    # Necessary for random sampling with DDP\n",
    "    if distributed:\n",
    "        train_sampler.set_epoch(e)\n",
    "\n",
    "    for images, masks, skeleton, skele_masks, baked in train_data:\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(enabled=mixed_precision):  # Saves Memory!\n",
    "            out: Tensor = model(images)\n",
    "\n",
    "            probability_map: Tensor = out[:, [-1], ...]\n",
    "            vector: Tensor = out[:, 0:3:1, ...]\n",
    "            predicted_skeleton: Tensor = out[:, [-2], ...]\n",
    "\n",
    "            embedding: Tensor = vector_to_embedding(num, vector)\n",
    "            out: Tensor = baked_embed_to_prob(embedding, baked, sigma(e))\n",
    "\n",
    "            _loss_embed = loss_embed(out, masks.gt(0).float())\n",
    "            _loss_prob = loss_prob(probability_map, masks.gt(0).float())\n",
    "            _loss_skeleton = loss_skele(predicted_skeleton, skele_masks.gt(\n",
    "                0).float())\n",
    "\n",
    "            # It sometimes is hard to learn all features at once. It can be beneficial to let the model learn the\n",
    "            # vectors/semantic mask first and then the skeleton\n",
    "            loss = _loss_embed + (1 * _loss_prob) + ((1 if e > 10 else 0) * _loss_skeleton)\n",
    "\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # For stochastic weight averaging\n",
    "        if e > swa_start:\n",
    "            swa_model.update_parameters(model)\n",
    "\n",
    "        _loss.append(loss.item())\n",
    "        _embed.append(_loss_embed.item())\n",
    "        _prob.append(_loss_prob.item())\n",
    "        _skele.append(_loss_skeleton.item())\n",
    "\n",
    "    # Avg epoch loss\n",
    "    avg_epoch_loss.append(mean(_loss))\n",
    "    avg_epoch_embed_loss.append(mean(_embed))\n",
    "    avg_epoch_prob_loss.append(mean(_prob))\n",
    "    avg_epoch_skele_loss.append(mean(_skele))\n",
    "\n",
    "    # update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # tensorboard writing\n",
    "    if writer and (rank == 0):\n",
    "        writer.add_scalar('lr', scheduler.get_last_lr()[-1], e)\n",
    "        writer.add_scalar('Loss/train', avg_epoch_loss[-1], e)\n",
    "        writer.add_scalar('Loss/embed', avg_epoch_embed_loss[-1], e)\n",
    "        writer.add_scalar('Loss/prob', avg_epoch_prob_loss[-1], e)\n",
    "        writer.add_scalar('Loss/skele-mask', avg_epoch_skele_loss[-1], e)\n",
    "        write_progress(writer=writer, tag='Train', epoch=e, images=images, masks=masks,\n",
    "                       probability_map=probability_map,\n",
    "                       vector=vector, out=out, skeleton=skeleton,\n",
    "                       predicted_skeleton=predicted_skeleton, gt_skeleton=skele_masks)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validation Step\n",
    "The validation step soely exists to write to tensorboard. No backprop. For speed we only do this once ever 10 epochs."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Validation Step\n",
    "# for e in trange(epochs): (FROM ABOVE!!!)\n",
    "    if e % 10 == 0 and val_data:\n",
    "        _loss, _embed, _prob, _skele = [], [], [], []\n",
    "        for images, masks, skeleton, skele_masks, baked in val_data:\n",
    "            with autocast(enabled=mixed_precision):  # Saves Memory!\n",
    "                with torch.no_grad():\n",
    "                    out: Tensor = swa_model(images)\n",
    "\n",
    "                    probability_map: Tensor = out[:, [-1], ...]\n",
    "                    predicted_skeleton: Tensor = out[:, [-2], ...]\n",
    "                    vector: Tensor = out[:, 0:3:1, ...]\n",
    "\n",
    "                    embedding: Tensor = vector_to_embedding(num, vector)\n",
    "                    out: Tensor = baked_embed_to_prob(embedding, baked, sigma(e))\n",
    "\n",
    "                    _loss_embed = loss_embed(out, masks.gt(0).float())\n",
    "                    _loss_prob = loss_prob(probability_map, masks.gt(0).float())\n",
    "                    _loss_skeleton = loss_prob(predicted_skeleton, skele_masks.gt(0).float())\n",
    "\n",
    "                    loss = (1 * _loss_embed) + (1 * _loss_prob) + _loss_skeleton\n",
    "\n",
    "            scaler.scale(loss)\n",
    "            _loss.append(loss.item())\n",
    "            _embed.append(_loss_embed.item())\n",
    "            _prob.append(_loss_prob.item())\n",
    "            _skele.append(_loss_skeleton.item())\n",
    "\n",
    "        avg_val_loss.append(mean(_loss))\n",
    "        avg_val_embed_loss.append(mean(_embed))\n",
    "        avg_val_prob_loss.append(mean(_prob))\n",
    "        avg_val_skele_loss.append(mean(_skele))\n",
    "\n",
    "        if writer and (rank == 0):\n",
    "            writer.add_scalar('Validation/train', avg_val_loss[-1], e)\n",
    "            writer.add_scalar('Validation/embed', avg_val_embed_loss[-1], e)\n",
    "            writer.add_scalar('Validation/prob', avg_val_prob_loss[-1], e)\n",
    "            write_progress(writer=writer, tag='Validation', epoch=e, images=images, masks=masks,\n",
    "                           probability_map=probability_map,\n",
    "                           vector=vector, out=out, skeleton=skeleton,\n",
    "                           predicted_skeleton=predicted_skeleton, gt_skeleton=skele_masks)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TQDM writing\n",
    "We use tqdm to get estimates on training speed and quick glance model loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for e in trange(epochs): (FROM ABOVE)\n",
    "    if rank == 0:\n",
    "        epoch_range.desc = f'lr={scheduler.get_last_lr()[-1]:.3e}, Loss (train | val): ' + f'{avg_epoch_loss[-1]:.5f} | {avg_val_loss[-1]:.5f}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Return the Model\n",
    "The training engine should return the model state dict, optimizer state dict, and the avg validation loss to be compatibvle with other scripts."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for e in trange(epochs):\n",
    "    # if rank == 0:\n",
    "        state_dict = model.module.state_dict() if hasattr(model, 'module') else model.state_dict()\n",
    "        if e % 100 == 0:\n",
    "            torch.save(state_dict, savepath + f'/test_{e}.trch')\n",
    "\n",
    "return state_dict, optimizer.state_dict(), avg_val_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## All Together\n",
    "Note - this is simply the entirety of ```skoots.train.engine```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def engine(\n",
    "        model: FasterRCNN,\n",
    "        lr: float,\n",
    "        wd: float,\n",
    "        vector_scale: Tuple[int, int, int],\n",
    "        epochs: int,\n",
    "        optimizer: Optimizer,\n",
    "        scheduler,\n",
    "        sigma: Sigma,\n",
    "        loss_embed,\n",
    "        loss_prob,\n",
    "        loss_skele,\n",
    "        device: str,\n",
    "        savepath: str,\n",
    "        train_data: Dataset,\n",
    "        rank: int,\n",
    "        val_data: Optional[Dataset] = None,\n",
    "        train_sampler=None,\n",
    "        test_sampler=None,\n",
    "        writer=None,\n",
    "        verbose=False,\n",
    "        distributed=True,\n",
    "        mixed_precision=False,\n",
    "        n_warmup: int = 100,\n",
    "        force=False,\n",
    "        **kwargs,\n",
    ") -> Tuple[OrderedDict, OrderedDict, List[float]]:\n",
    "\n",
    "    # Print out each kwarg to std out\n",
    "    if verbose and rank == 0:\n",
    "        print('Initiating Training Run', flush=False)\n",
    "        vars = locals()\n",
    "        for k in vars:\n",
    "            if k != 'model':\n",
    "                print(f'\\t> {k}: {vars[k]}', flush=False)\n",
    "        print('', flush=True)\n",
    "\n",
    "    num = torch.tensor(vector_scale, device=device)\n",
    "\n",
    "\n",
    "    optimizer = optimizer(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    scheduler = scheduler(optimizer)\n",
    "    scaler = GradScaler(enabled=mixed_precision)\n",
    "\n",
    "    swa_model = torch.optim.swa_utils.AveragedModel(model)\n",
    "    swa_start = 100\n",
    "    swa_scheduler = torch.optim.swa_utils.SWALR(optimizer, swa_lr=0.05)\n",
    "\n",
    "    # Save each loss value in a list...\n",
    "    avg_epoch_loss = []\n",
    "    avg_epoch_embed_loss = []\n",
    "    avg_epoch_prob_loss = []\n",
    "    avg_epoch_skele_loss = []\n",
    "\n",
    "    avg_val_loss = []\n",
    "    avg_val_embed_loss = []\n",
    "    avg_val_prob_loss = []\n",
    "    avg_val_skele_loss = []\n",
    "\n",
    "    # skel_crossover_loss = skoots.train.loss.split(n_iter=3, alpha=2)\n",
    "\n",
    "    # Warmup... Get the first from train_data\n",
    "    for images, masks, skeleton, skele_masks, baked in train_data:\n",
    "        pass\n",
    "\n",
    "    warmup_range = trange(n_warmup, desc='Warmup: {}')\n",
    "    for w in warmup_range:\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(enabled=mixed_precision):  # Saves Memory!\n",
    "            out: Tensor = model(images)\n",
    "\n",
    "            probability_map: Tensor = out[:, [-1], ...]\n",
    "            vector: Tensor = out[:, 0:3:1, ...]\n",
    "            predicted_skeleton: Tensor = out[:, [-2], ...]\n",
    "\n",
    "            embedding: Tensor = vector_to_embedding(num, vector)\n",
    "            out: Tensor = baked_embed_to_prob(embedding, baked, sigma(0))\n",
    "\n",
    "            _loss_embed = loss_embed(out, masks.gt(0).float())  # out = [B, 2/3, X, Y, Z?]\n",
    "            _loss_prob = loss_prob(probability_map, masks.gt(0).float())\n",
    "            _loss_skeleton = loss_skele(predicted_skeleton, skele_masks.gt(\n",
    "                0).float())  # + skel_crossover_loss(predicted_skeleton, skele_masks.gt(0).float())\n",
    "            loss = _loss_embed + (1 * _loss_prob) + (1 * _loss_skeleton)\n",
    "\n",
    "            # print('All Skeleton Loss: ', _loss_skeleton.item())\n",
    "            # print('Skeleton Loss of just crossover: ',\n",
    "            #       skel_crossover_loss(predicted_skeleton, skele_masks.gt(0).float()))\n",
    "\n",
    "            warmup_range.desc = f'{loss.item()}'\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                print(f'Found NaN value in loss.\\n\\tLoss Embed: {_loss_embed}\\n\\tLoss Probability: {_loss_prob}')\n",
    "                print(f'\\t{torch.any(torch.isnan(vector))}')\n",
    "                print(f'\\t{torch.any(torch.isnan(embedding))}')\n",
    "                continue\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "    # Train Step...\n",
    "    epoch_range = trange(epochs, desc=f'Loss = {1.0000000}') if rank == 0 else range(epochs)\n",
    "    for e in epoch_range:\n",
    "        _loss, _embed, _prob, _skele = [], [], [], []\n",
    "\n",
    "        if distributed:\n",
    "            train_sampler.set_epoch(e)\n",
    "\n",
    "        for images, masks, skeleton, skele_masks, baked in train_data:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            with autocast(enabled=mixed_precision):  # Saves Memory!\n",
    "                out: Tensor = model(images)\n",
    "\n",
    "                probability_map: Tensor = out[:, [-1], ...]\n",
    "                vector: Tensor = out[:, 0:3:1, ...]\n",
    "                predicted_skeleton: Tensor = out[:, [-2], ...]\n",
    "\n",
    "                embedding: Tensor = vector_to_embedding(num, vector)\n",
    "                out: Tensor = baked_embed_to_prob(embedding, baked, sigma(e))\n",
    "\n",
    "                _loss_embed = loss_embed(out, masks.gt(0).float())  # out = [B, 2/3, X, Y, :w\n",
    "                # Z?]\n",
    "                _loss_prob = loss_prob(probability_map, masks.gt(0).float())\n",
    "                _loss_skeleton = loss_skele(predicted_skeleton, skele_masks.gt(\n",
    "                    0).float())  # + skel_crossover_loss(predicted_skeleton, skele_masks.gt(0).float())\n",
    "\n",
    "                loss = _loss_embed + (1 * _loss_prob) + ((1 if e > 10 else 0) * _loss_skeleton)\n",
    "\n",
    "\n",
    "\n",
    "                if torch.isnan(loss):\n",
    "                    print(f'Found NaN value in loss.\\n\\tLoss Embed: {_loss_embed}\\n\\tLoss Probability: {_loss_prob}')\n",
    "                    print(f'\\t{torch.any(torch.isnan(vector))}')\n",
    "                    print(f'\\t{torch.any(torch.isnan(embedding))}')\n",
    "                    continue\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            if e > swa_start:\n",
    "                swa_model.update_parameters(model)\n",
    "\n",
    "            _loss.append(loss.item())\n",
    "            _embed.append(_loss_embed.item())\n",
    "            _prob.append(_loss_prob.item())\n",
    "            _skele.append(_loss_skeleton.item())\n",
    "\n",
    "        avg_epoch_loss.append(mean(_loss))\n",
    "        avg_epoch_embed_loss.append(mean(_embed))\n",
    "        avg_epoch_prob_loss.append(mean(_prob))\n",
    "        avg_epoch_skele_loss.append(mean(_skele))\n",
    "        scheduler.step()\n",
    "\n",
    "        if writer and (rank == 0):\n",
    "            writer.add_scalar('lr', scheduler.get_last_lr()[-1], e)\n",
    "            writer.add_scalar('Loss/train', avg_epoch_loss[-1], e)\n",
    "            writer.add_scalar('Loss/embed', avg_epoch_embed_loss[-1], e)\n",
    "            writer.add_scalar('Loss/prob', avg_epoch_prob_loss[-1], e)\n",
    "            writer.add_scalar('Loss/skele-mask', avg_epoch_skele_loss[-1], e)\n",
    "            write_progress(writer=writer, tag='Train', epoch=e, images=images, masks=masks,\n",
    "                           probability_map=probability_map,\n",
    "                           vector=vector, out=out, skeleton=skeleton,\n",
    "                           predicted_skeleton=predicted_skeleton, gt_skeleton=skele_masks)\n",
    "\n",
    "        # # Validation Step\n",
    "        if e % 10 == 0 and val_data:\n",
    "            _loss, _embed, _prob, _skele = [], [], [], []\n",
    "            for images, masks, skeleton, skele_masks, baked in val_data:\n",
    "                with autocast(enabled=mixed_precision):  # Saves Memory!\n",
    "                    with torch.no_grad():\n",
    "                        out: Tensor = swa_model(images)\n",
    "\n",
    "                        probability_map: Tensor = out[:, [-1], ...]\n",
    "                        predicted_skeleton: Tensor = out[:, [-2], ...]\n",
    "                        vector: Tensor = out[:, 0:3:1, ...]\n",
    "\n",
    "                        embedding: Tensor = vector_to_embedding(num, vector)\n",
    "                        out: Tensor = baked_embed_to_prob(embedding, baked, sigma(e))\n",
    "\n",
    "                        _loss_embed = loss_embed(out, masks.gt(0).float())\n",
    "                        _loss_prob = loss_prob(probability_map, masks.gt(0).float())\n",
    "                        _loss_skeleton = loss_prob(predicted_skeleton, skele_masks.gt(0).float())\n",
    "\n",
    "                        loss = (2 * _loss_embed) + (2 * _loss_prob) + _loss_skeleton\n",
    "\n",
    "                        if torch.isnan(loss):\n",
    "                            print(\n",
    "                                f'Found NaN value in loss.\\n\\tLoss Embed: {_loss_embed}\\n\\tLoss Probability: {_loss_prob}')\n",
    "                            print(f'\\t{torch.any(torch.isnan(vector))}')\n",
    "                            print(f'\\t{torch.any(torch.isnan(embedding))}')\n",
    "                            continue\n",
    "\n",
    "                scaler.scale(loss)\n",
    "                _loss.append(loss.item())\n",
    "                _embed.append(_loss_embed.item())\n",
    "                _prob.append(_loss_prob.item())\n",
    "                _skele.append(_loss_skeleton.item())\n",
    "\n",
    "            avg_val_loss.append(mean(_loss))\n",
    "            avg_val_embed_loss.append(mean(_embed))\n",
    "            avg_val_prob_loss.append(mean(_prob))\n",
    "            avg_val_skele_loss.append(mean(_skele))\n",
    "\n",
    "            if writer and (rank == 0):\n",
    "                writer.add_scalar('Validation/train', avg_val_loss[-1], e)\n",
    "                writer.add_scalar('Validation/embed', avg_val_embed_loss[-1], e)\n",
    "                writer.add_scalar('Validation/prob', avg_val_prob_loss[-1], e)\n",
    "                write_progress(writer=writer, tag='Validation', epoch=e, images=images, masks=masks,\n",
    "                               probability_map=probability_map,\n",
    "                               vector=vector, out=out, skeleton=skeleton,\n",
    "                               predicted_skeleton=predicted_skeleton, gt_skeleton=skele_masks)\n",
    "\n",
    "        if rank == 0:\n",
    "            epoch_range.desc = f'lr={scheduler.get_last_lr()[-1]:.3e}, Loss (train | val): ' + f'{avg_epoch_loss[-1]:.5f} | {avg_val_loss[-1]:.5f}'\n",
    "\n",
    "        state_dict = model.module.state_dict() if hasattr(model, 'module') else model.state_dict()\n",
    "        if e % 100 == 0:\n",
    "            torch.save(state_dict, savepath + f'/test_{e}.trch')\n",
    "\n",
    "        return state_dict, optimizer.state_dict(), avg_val_loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}