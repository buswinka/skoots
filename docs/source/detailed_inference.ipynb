{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```{eval-rst}\n",
    ".. role:: nge-yellow\n",
    "```\n",
    "{nge-yellow}`Detailed Inference Protocol`\n",
    "==================================="
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports\n",
    "Inference is handled by the ```eval()``` function in ```skoots.lib.eval.py```. We first need to import all requred libraries, notably zarr, fastremap, numpy, and torch."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import logging\n",
    "import os.path\n",
    "import tracemalloc\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "import fastremap\n",
    "import zarr\n",
    "\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "\n",
    "import torch\n",
    "import torch._dynamo\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.cuda.amp import autocast\n",
    "from tqdm import tqdm\n",
    "from yacs.config import CfgNode\n",
    "\n",
    "import skoots.lib.skeleton\n",
    "from skoots.lib.cropper import crops\n",
    "from skoots.lib.flood_fill import efficient_flood_fill\n",
    "from skoots.lib.morphology import binary_dilation, binary_dilation_2d\n",
    "from skoots.lib.utils import cfg_to_bism_model\n",
    "from skoots.lib.vector_to_embedding import vector_to_embedding\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the image and model\n",
    "SKOOTS performs best when evaluated with the same core parameters used for training. Therefore, we package these with the model file, such that they never need to be remembered. This takes the form of a YACS config node. We therefore load this, the model, and the input image."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@torch.inference_mode()  # disables autograd and reference counting for SPEED\n",
    "def eval(\n",
    "    image_path: str,\n",
    "    checkpoint_path: str = \"/home/chris/Dropbox (Partners HealthCare)/trainMitochondriaSegmentation/models/Oct21_17-15-08_CHRISUBUNTU.trch\",\n",
    ") -> None:\n",
    "    tracemalloc.start()\n",
    "    start = time.time()\n",
    "\n",
    "    torch._dynamo.config.log_level = logging.ERROR\n",
    "    logging.info(f\"Loading model file: {checkpoint_path}\")\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    if \"cfg\" in checkpoint:\n",
    "        cfg: CfgNode = checkpoint[\"cfg\"]\n",
    "    else:\n",
    "        raise RuntimeError(\"Attempting to evaluate skoots on a legacy model file.\")\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    filename_without_extensions = os.path.splitext(image_path)[0]\n",
    "\n",
    "    # make sure the image is 5 channels, in [C, X, Y, Z] shape, and normalized between 0 and 1\n",
    "    logging.info(f\"Loading image from file: {image_path}\")\n",
    "    image: np.array = io.imread(image_path)  # [Z, X, Y, C]\n",
    "    image: np.array = image[..., np.newaxis] if image.ndim == 3 else image\n",
    "    image: np.array = image.transpose(-1, 1, 2, 0)\n",
    "    image: np.array = image[[2], ...] if image.shape[0] > 3 else image  # [C=1, X, Y, Z]\n",
    "\n",
    "    scale: int = 2**16 if image.dtype == np.uint16 else 2**8\n",
    "    image: Tensor = torch.from_numpy(image).pin_memory()\n",
    "\n",
    "    vector_scale = torch.tensor(cfg.SKOOTS.VECTOR_SCALING)\n",
    "\n",
    "    # we use bism for constructing the models.\n",
    "    logging.info(f\"Constructing SKOOTS model\")\n",
    "    base_model: nn.Module = cfg_to_bism_model(cfg)  # This is our skoots torch model\n",
    "    base_model.load_state_dict(state_dict=checkpoint[\"model_state_dict\"])\n",
    "    base_model = base_model.to(device).train()\n",
    "\n",
    "    logging.info(f\"Compiling SKOOTS model with torch inductor\")\n",
    "    model = torch.compile(base_model)\n",
    "    for _ in range(10):  # warmup torchinductor\n",
    "        _ = model(torch.rand((1, 1, 300, 300, 20), device=device, dtype=torch.float))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preallocate intermediary arrays\n",
    "For inference, SKOOTS needs to keep track of the skeleton and embedding vectors. We can pre-allocate them here."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    c, x, y, z = image.shape\n",
    "    skeleton = torch.zeros(size=(1, x, y, z), dtype=torch.int16)\n",
    "    vectors = torch.zeros((3, x, y, z), dtype=torch.half)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Iterative Evaluation\n",
    "It is likely the deep neural network model cannot process the entire image at once. Rather, we crop the image using a cropping utility in ```skoots.lib.cropper.crops```.  This simply creates a generator which returns a crop, and it's index. I have been tempted to increase the crop size, it somehow leads to degraded performance. I'd keep it at the default."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    cropsize = [300, 300, 20]  # DEFAULT (300, 300, 20), If you change it, the model might screw up!!!\n",
    "    overlap = [10, 10, 5]\n",
    "\n",
    "    total = skoots.lib.cropper.get_total_num_crops(image.shape, cropsize, overlap)\n",
    "    iterator = tqdm(\n",
    "        crops(image, cropsize, overlap, device=device), desc=\"\", total=total\n",
    "    )\n",
    "    benchmark_start = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now loop over the crop and evaluate the model. To reduce the amount of storage, we only take the vectors and skeletons wich are likely in an object, as defined by the probability map."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    for crop, (x, y, z) in iterator:\n",
    "        with autocast(enabled=True):  # Saves Memory!\n",
    "            out = model(crop.div(scale).float().cuda())\n",
    "\n",
    "        probability_map = out[:, [-1], ...]\n",
    "        skeleton_map = out[:, [-2], ...].float()\n",
    "        vec = out[:, 0:3:1, ...]\n",
    "\n",
    "        vec = vec * probability_map.gt(0.5)\n",
    "        skeleton_map = skeleton_map * probability_map.gt(0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have found that performing binary expansion on the skeletons in 2d/3d helps with overall accuraccy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "        for _ in range(\n",
    "            1\n",
    "        ):  # expand the skeletons in x/y/z. Only  because they can get too skinny\n",
    "            skeleton_map = binary_dilation(skeleton_map)\n",
    "            for _ in range(3):  # expand 2 times just in x/y\n",
    "                skeleton_map = binary_dilation_2d(skeleton_map)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now store the crop in the buffer."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "        # put the predictions into the preallocated tensors...\n",
    "        _destination = (\n",
    "            ...,\n",
    "            slice(x + overlap[0], x + cropsize[0] - overlap[0]),\n",
    "            slice(y + overlap[1], y + cropsize[1] - overlap[1]),\n",
    "            slice(z + overlap[2], z + cropsize[2] - overlap[2]),\n",
    "        )\n",
    "\n",
    "        _source = (\n",
    "            0,\n",
    "            ...,\n",
    "            slice(overlap[0], -overlap[0]),\n",
    "            slice(overlap[1], -overlap[1]),\n",
    "            slice(overlap[2], -overlap[2]),\n",
    "        )\n",
    "\n",
    "        skeleton[_destination] = skeleton_map[_source].gt(0.8).cpu()\n",
    "        vectors[_destination] = vec[_source].half().cpu()\n",
    "\n",
    "        iterator.desc = f\"Evaluating UNet on slice [x{x}:y{y}:z{z}]\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Assigning ID's\n",
    "Once we have saved the entire skeleton and vectors, we now must assign an id to the skeletons. This is done via flood fill from ```skoots.lib.floot_fill.py```. It's not that efficient."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    skeleton: Tensor = efficient_flood_fill(skeleton)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate Instance Masks\n",
    "From the labeled skeletons, we can get instance masks using the embeddings. We do this via crops as well, using similar functions from training."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    cropsize = [500, 500, 50]\n",
    "    overlap = (50, 50, 5)\n",
    "    iterator = tqdm(\n",
    "        crops(vectors, crop_size=cropsize, overlap=overlap), desc=\"Assigning Instances:\"\n",
    "    )\n",
    "\n",
    "    instance_mask = torch.zeros_like(skeleton, dtype=torch.int16)\n",
    "    skeleton = skeleton.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    logging.info(f\"Identifying connected components...\")\n",
    "    for _vec, (x, y, z) in iterator:\n",
    "        _destination = (\n",
    "            slice(x + overlap[0], x + cropsize[0] - overlap[0]),\n",
    "            slice(y + overlap[1], y + cropsize[1] - overlap[1]),\n",
    "            slice(z + overlap[2], z + cropsize[2] - overlap[2]),\n",
    "        )\n",
    "\n",
    "        _source = (\n",
    "            slice(overlap[0], -overlap[0]),\n",
    "            slice(overlap[1], -overlap[1]),\n",
    "            slice(overlap[2], -overlap[2]),\n",
    "        )\n",
    "\n",
    "        _embed = skoots.lib.vector_to_embedding.vector_to_embedding(\n",
    "            scale=vector_scale, vector=_vec, N=2\n",
    "        )\n",
    "        _embed += torch.tensor((x, y, z)).view(\n",
    "            1, 3, 1, 1, 1\n",
    "        )  # We adjust embedding to region of the crop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unlike training, we dont know where these embeddings are supposed to be. Rather we trust they are pointing into a skeleton. Therefore, to assign an instance label, we let an embedding point to a labeled skeleton, which assigns its label. This is simply an indexing operation, and is therefore fast."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "        # This gives the instance mask!\n",
    "        _inst_maks = skoots.lib.skeleton.index_skeleton_by_embed(\n",
    "            skeleton=skeleton, embed=_embed\n",
    "        ).squeeze()\n",
    "\n",
    "        # Plop it back into the pre-allocated array\n",
    "        instance_mask[_destination] = (\n",
    "            _inst_maks[_source] if torch.tensor(overlap).gt(0).all() else _inst_maks\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}